"""
üöÄ HessGPT Web Interface - Flask Server
‚úÖ Compatible avec ton mod√®le Epoch 6 (meilleur)
‚úÖ API REST + Interface HTML
‚úÖ Pr√™t pour production
"""

from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
import torch
import torch.nn.functional as F
from transformers import GPT2Tokenizer
import sys
import os

# Importer ton mod√®le
sys.path.append('./Core/Model')
from HessGpt import HessGPT

app = Flask(__name__)
CORS(app)

# ============================================
# CONFIGURATION
# ============================================

CONFIG = {
    'vocab_size': 50257,
    'embed_dim': 512,
    'num_heads': 8,
    'num_layers': 8,
    'max_seq_len': 1024,
    'dropout': 0.05,
}

# Chemins des checkpoints (ordre de pr√©f√©rence)
CHECKPOINT_PATHS = [
    "./checkpoints/quality/hessgpt_sft_quality_BEST.pt",
    "./checkpoints/quality/hessgpt_sft_RESUME.pt"
]

# Device
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# ============================================
# INITIALISATION MOD√àLE
# ============================================

print("="*60)
print("üöÄ D√âMARRAGE SERVEUR HessGPT")
print("="*60)
print(f"‚úÖ Device: {DEVICE}")

# Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Chargement mod√®le
checkpoint = None
for path in CHECKPOINT_PATHS:
    if os.path.exists(path):
        try:
            checkpoint = torch.load(path, map_location=DEVICE)
            print(f"‚úì Checkpoint charg√©: {path.split('/')[-1]}")
            print(f"‚úì Epoch: {checkpoint.get('epoch', 'N/A')}")
            print(f"‚úì Val Loss: {checkpoint.get('val_loss', checkpoint.get('best_val_loss', 'N/A')):.4f}")
            break
        except Exception as e:
            print(f"‚úó Erreur chargement {path}: {e}")
            continue

if checkpoint is None:
    print("‚ùå ERREUR: Aucun checkpoint trouv√©!")
    print("üìÅ Cherch√© dans:")
    for path in CHECKPOINT_PATHS:
        print(f"  - {path}")
    sys.exit(1)

# Initialisation mod√®le
model = HessGPT(**CONFIG).to(DEVICE)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

print("‚úÖ Mod√®le charg√© et pr√™t!")
print("="*60)

# ============================================
# FONCTION DE G√âN√âRATION
# ============================================

def generate_response(prompt, max_tokens=100, temperature=0.7, top_k=50, top_p=0.9):
    """
    G√©n√®re une r√©ponse avec le mod√®le HessGPT
    
    Args:
        prompt: Texte d'entr√©e
        max_tokens: Nombre maximum de tokens √† g√©n√©rer
        temperature: Contr√¥le la cr√©ativit√© (0.1-1.0)
        top_k: Nombre de meilleurs tokens √† consid√©rer
        top_p: Nucleus sampling threshold
    
    Returns:
        str: R√©ponse g√©n√©r√©e
    """
    model.eval()
    
    # Formater le prompt (style Alpaca)
    formatted_prompt = f"Instruction: {prompt}\nResponse:"
    
    # Tokenization
    tokens = tokenizer.encode(formatted_prompt, return_tensors='pt').to(DEVICE)
    generated = tokens[0].tolist()
    
    with torch.no_grad():
        for _ in range(max_tokens):
            # Forward pass
            input_ids = torch.tensor([generated], dtype=torch.long).to(DEVICE)
            
            # Limite la longueur du contexte si n√©cessaire
            if input_ids.size(1) > CONFIG['max_seq_len']:
                input_ids = input_ids[:, -CONFIG['max_seq_len']:]
            
            logits, _ = model(input_ids)
            next_token_logits = logits[0, -1, :]
            
            # Temp√©rature
            next_token_logits = next_token_logits / temperature
            
            # Anti-r√©p√©tition (p√©nalise les tokens r√©cents)
            for token in set(generated[-50:]):
                next_token_logits[token] /= 1.2
            
            # Top-k filtering
            if top_k > 0:
                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # Top-p (nucleus) filtering
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # Sampling
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1).item()
            
            # Stop si EOS
            if next_token == tokenizer.eos_token_id:
                break
            
            generated.append(next_token)
    
    # D√©coder et extraire la r√©ponse
    full_text = tokenizer.decode(generated, skip_special_tokens=True)
    
    # Extraire uniquement la partie "Response:"
    if "Response:" in full_text:
        response = full_text.split("Response:")[-1].strip()
    else:
        response = full_text[len(formatted_prompt):].strip()
    
    return response

# ============================================
# ROUTES FLASK
# ============================================

@app.route('/')
def home():
    """Page d'accueil avec l'interface"""
    return render_template('index.html')

@app.route('/generate', methods=['POST'])
def generate():
    """
    API de g√©n√©ration
    
    Body JSON:
    {
        "prompt": "Votre question",
        "max_tokens": 100,
        "temperature": 0.7
    }
    
    Returns:
    {
        "response": "R√©ponse g√©n√©r√©e",
        "success": true
    }
    """
    try:
        data = request.get_json()
        
        # Validation
        if not data or 'prompt' not in data:
            return jsonify({
                'error': 'Prompt manquant',
                'success': False
            }), 400
        
        prompt = data['prompt'].strip()
        if not prompt:
            return jsonify({
                'error': 'Prompt vide',
                'success': False
            }), 400
        
        # Param√®tres avec valeurs par d√©faut
        max_tokens = min(int(data.get('max_tokens', 100)), 500)  # Max 500
        temperature = max(0.1, min(float(data.get('temperature', 0.7)), 1.0))  # Entre 0.1 et 1.0
        
        # G√©n√©ration
        response = generate_response(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        return jsonify({
            'response': response,
            'success': True,
            'params': {
                'max_tokens': max_tokens,
                'temperature': temperature
            }
        })
    
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration: {e}")
        return jsonify({
            'error': f'Erreur serveur: {str(e)}',
            'success': False
        }), 500

@app.route('/clear', methods=['POST'])
def clear():
    """Efface l'historique (pour compatibilit√© frontend)"""
    return jsonify({'success': True, 'message': 'Conversation effac√©e'})

@app.route('/health', methods=['GET'])
def health():
    """Health check pour monitoring"""
    return jsonify({
        'status': 'healthy',
        'device': DEVICE,
        'model': 'HessGPT',
        'epoch': checkpoint.get('epoch', 'N/A'),
        'val_loss': checkpoint.get('val_loss', checkpoint.get('best_val_loss', 'N/A'))
    })

@app.route('/info', methods=['GET'])
def info():
    """Informations sur le mod√®le"""
    return jsonify({
        'model': 'HessGPT',
        'version': '1.0',
        'epoch': checkpoint.get('epoch', 'N/A'),
        'val_loss': checkpoint.get('val_loss', checkpoint.get('best_val_loss', 'N/A')),
        'config': {
            'vocab_size': CONFIG['vocab_size'],
            'embed_dim': CONFIG['embed_dim'],
            'num_layers': CONFIG['num_layers'],
            'num_heads': CONFIG['num_heads'],
            'max_seq_len': CONFIG['max_seq_len']
        },
        'device': DEVICE,
        'samples_seen': checkpoint.get('total_samples_seen', 'N/A')
    })

# ============================================
# D√âMARRAGE SERVEUR
# ============================================

if __name__ == '__main__':
    print("\n" + "="*60)
    print("üåê Serveur d√©marr√©!")
    print("="*60)
    print("üìç Interface: http://localhost:5000")
    print("üìç API: http://localhost:5000/generate")
    print("üìç Health: http://localhost:5000/health")
    print("üìç Info: http://localhost:5000/info")
    print("="*60)
    print("\n‚ö†Ô∏è  Utiliser CTRL+C pour arr√™ter\n")
    
    # D√©marrage (mode debug pour d√©veloppement)
    app.run(
        host='0.0.0.0',  # Accessible depuis r√©seau local
        port=5000,
        debug=False,     # Mettre True en dev, False en prod
        threaded=True
    )